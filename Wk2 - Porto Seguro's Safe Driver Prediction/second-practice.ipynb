{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n아래 코드를 참고하여 작성한 필사입니다.  \nhttps://www.kaggle.com/bertcarremans/data-preparation-exploration#Feature-scaling  \nhttps://www.kaggle.com/sihwanyoon/data-preparation-exploration  \nhttps://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features  \n\n## Loading packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\nImputer = SimpleImputer(strategy='mean')\n# Imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-03T10:34:30.722664Z","iopub.execute_input":"2021-11-03T10:34:30.723173Z","iopub.status.idle":"2021-11-03T10:34:30.732306Z","shell.execute_reply.started":"2021-11-03T10:34:30.723115Z","shell.execute_reply":"2021-11-03T10:34:30.731126Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:30.734053Z","iopub.execute_input":"2021-11-03T10:34:30.735213Z","iopub.status.idle":"2021-11-03T10:34:39.896926Z","shell.execute_reply.started":"2021-11-03T10:34:30.735165Z","shell.execute_reply":"2021-11-03T10:34:39.893160Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# 1. Visual inspection of data\n## Data at first sight\nexcerpt of data description for the competition:\n- Features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc).\n- Feature names include the postfix bin to indicate binary features and cat to indicate categorical features.\n- Features without these designations are either continuous or ordinal.\n- Values of -1 indicate that the feature was missing from the observation.\n- The target columns signifies whether or not a claim was filed for that policy holder.\nThis is important information for us to get started.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:39.901563Z","iopub.execute_input":"2021-11-03T10:34:39.902376Z","iopub.status.idle":"2021-11-03T10:34:40.067158Z","shell.execute_reply.started":"2021-11-03T10:34:39.902255Z","shell.execute_reply":"2021-11-03T10:34:40.063919Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:40.070624Z","iopub.execute_input":"2021-11-03T10:34:40.071880Z","iopub.status.idle":"2021-11-03T10:34:40.179079Z","shell.execute_reply.started":"2021-11-03T10:34:40.071756Z","shell.execute_reply":"2021-11-03T10:34:40.178189Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"We see:\n- binary variables\n- categorical variables (category values are integers)\n- other variables w/ int for float values\n- variables w/ -1 representing missing values\n- target variable & ID ","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:40.184464Z","iopub.execute_input":"2021-11-03T10:34:40.185208Z","iopub.status.idle":"2021-11-03T10:34:40.198493Z","shell.execute_reply.started":"2021-11-03T10:34:40.185024Z","shell.execute_reply":"2021-11-03T10:34:40.197262Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"#### rows and columns in the train data\n- 59 variables  \n- 595212 rows  \n\nLet's see if we have the same number of variables in the test data, and if there are duplicate rows in the training data.","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:40.200788Z","iopub.execute_input":"2021-11-03T10:34:40.201881Z","iopub.status.idle":"2021-11-03T10:34:41.876041Z","shell.execute_reply.started":"2021-11-03T10:34:40.201824Z","shell.execute_reply":"2021-11-03T10:34:41.875113Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are no duplicate rows. :)","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:41.877469Z","iopub.execute_input":"2021-11-03T10:34:41.877734Z","iopub.status.idle":"2021-11-03T10:34:41.884779Z","shell.execute_reply.started":"2021-11-03T10:34:41.877703Z","shell.execute_reply":"2021-11-03T10:34:41.883788Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"We are missing one variable in the test set, but this is the target variable so it's ok.\n\nNow let's see how many variables of each type we have.\n\nLater on we can create dummy variables for the 14 categorical variables.  \n__bin__ -> variable, already binary and do not need dummification","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:41.886181Z","iopub.execute_input":"2021-11-03T10:34:41.886868Z","iopub.status.idle":"2021-11-03T10:34:41.974187Z","shell.execute_reply.started":"2021-11-03T10:34:41.886809Z","shell.execute_reply":"2021-11-03T10:34:41.973111Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"__info()__ -> method, gives info about variables  \nWe can see that the data type is int or float.  \nThere are no null values present in the data set.\nThis is normal because missing values are replaced by 01.\nWe will look into that later.\n\n# 2. Defining the metadata\n\n## Metadata\nTo facilitate the data management, we'll store meta-information about the variables in a DataFrame.  \nThis will be helpful when we want to select specific variables for analysis, visualization, modeling, etc.\n\nConcretely we will store:\n- __role__: input, ID, target\n- __level__: nominal, interval, ordinal, binary\n- __keep__: True, False\n- __dtype__: int, float, str","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    \n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # Defining the data type\n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname' : f,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(f_dict)\n\nmeta = pd.DataFrame(data, columns = ['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:41.976005Z","iopub.execute_input":"2021-11-03T10:34:41.976259Z","iopub.status.idle":"2021-11-03T10:34:41.991156Z","shell.execute_reply.started":"2021-11-03T10:34:41.976225Z","shell.execute_reply":"2021-11-03T10:34:41.990316Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:41.992360Z","iopub.execute_input":"2021-11-03T10:34:41.993451Z","iopub.status.idle":"2021-11-03T10:34:42.027353Z","shell.execute_reply.started":"2021-11-03T10:34:41.993393Z","shell.execute_reply":"2021-11-03T10:34:42.026288Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Below is an example of extracting all nominal variables that are not dropped","metadata":{}},{"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:42.028829Z","iopub.execute_input":"2021-11-03T10:34:42.029212Z","iopub.status.idle":"2021-11-03T10:34:42.040969Z","shell.execute_reply.started":"2021-11-03T10:34:42.029180Z","shell.execute_reply":"2021-11-03T10:34:42.039729Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'count': meta.groupby(['role','level'])['role'].size()}).reset_index()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:42.043071Z","iopub.execute_input":"2021-11-03T10:34:42.043630Z","iopub.status.idle":"2021-11-03T10:34:42.067187Z","shell.execute_reply.started":"2021-11-03T10:34:42.043583Z","shell.execute_reply":"2021-11-03T10:34:42.065606Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"# 3. Descriptive statistics\nWe can also apply the __describe__ method on the dataframe.  \nHowever, it doesn't make much sense to calculate the mean, std, etc. on categorical variables and the id variable.\nWe'll explore the categorical variables visually later.\n\nThanks to our meta file we can easily select the variables on which we want to compute the descriptive statisctics.\nTo keep things clear, we'll do this per data type.\n\n## Interval variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:42.069571Z","iopub.execute_input":"2021-11-03T10:34:42.070170Z","iopub.status.idle":"2021-11-03T10:34:42.424711Z","shell.execute_reply.started":"2021-11-03T10:34:42.070082Z","shell.execute_reply":"2021-11-03T10:34:42.423677Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### reg variables\n- only ps_reg_03 has missing values (-1)\n- range (min to max) differs between the variables. We can apply __scaling (ex. StandardScaler)__, but it depends on the clasifier we will want to use\n\n### car variables\n- ps_car_12 & ps_car_15 has missing values\n- range differs. we can apply scaling\n\n### calc variables\n- no missing values\n- this seems to be some kind of ratio as the max is 0.9\n- all three variables have similar distributions\n\n#### Overall,\nwe can see that the range of the interval variables is rather small. Perhaps some transformation (ex.log) is already applied in order to anonymize the data? ?\n\n## Ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:42.429717Z","iopub.execute_input":"2021-11-03T10:34:42.430367Z","iopub.status.idle":"2021-11-03T10:34:42.817677Z","shell.execute_reply.started":"2021-11-03T10:34:42.430320Z","shell.execute_reply":"2021-11-03T10:34:42.816702Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"- only one missing variable; ps_car_11\n- we can apply scaling to deal with the different ranges\n\n## Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:42.819454Z","iopub.execute_input":"2021-11-03T10:34:42.819795Z","iopub.status.idle":"2021-11-03T10:34:43.205490Z","shell.execute_reply.started":"2021-11-03T10:34:42.819751Z","shell.execute_reply":"2021-11-03T10:34:43.204258Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"- A priori in the train data is 3.645%, which is strongly imbalanced.\n- From the means we can conclude that for most variables the value is zero in most cases (because they are so close to 0)\n\n# 4. Handling imbalanced classes\nAs we mentioned above the proportion of records w/ target = 1 is far less than target = 0.  \nThis can lead to a model that has great accuracy but does have any added value in practice.  \nTwo possible strategies to deal with this problem are:\n- oversampling records w/ target = 1\n- undersampling records w/ target = 0  \n\nThere are many more strategies, but as we have a rather large training set, we can go for __undersampling__","metadata":{}},{"cell_type":"code","source":"desired_apriori = 0.10\n\n# Get the indicesper target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target = 0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate * nb_0)\nprint('Rate to undersample records with target = 0: {}'.format(undersampling_rate))\nprint('Number of records with target = 0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target = 0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state = 37, n_samples = undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:43.207415Z","iopub.execute_input":"2021-11-03T10:34:43.208189Z","iopub.status.idle":"2021-11-03T10:34:44.028839Z","shell.execute_reply.started":"2021-11-03T10:34:43.208132Z","shell.execute_reply":"2021-11-03T10:34:44.027482Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Quality Checks\n\n## Checking missing values\n(they are, again, represencted as -1)","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings / train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.030748Z","iopub.execute_input":"2021-11-03T10:34:44.031095Z","iopub.status.idle":"2021-11-03T10:34:44.218279Z","shell.execute_reply.started":"2021-11-03T10:34:44.031053Z","shell.execute_reply":"2021-11-03T10:34:44.216957Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"- ps_car_03_cat & ps_car_05_cat have a large proportion of records with missing values. We have to remove these variables.\n- For the other categorical variables with missing values, we can leave the missing value -1 as such.\n- ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.\n- ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.\n- ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n- ps_car_14 (continuous) has missing values for 7% of all records. Replace by the mean.","metadata":{}},{"cell_type":"code","source":"# 너무 많은 결측값을 지닌 Feature들을 제거합니다 (68.4%, 44.3%)\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # 메타데이터를 업데이트해줍니다.\n\n# 결측값을 Imputer를 활용하여 변환해줍니다.\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.220032Z","iopub.execute_input":"2021-11-03T10:34:44.220271Z","iopub.status.idle":"2021-11-03T10:34:44.323443Z","shell.execute_reply.started":"2021-11-03T10:34:44.220242Z","shell.execute_reply":"2021-11-03T10:34:44.322794Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Checking the cardinality of the categorical variables\n### Cardinality \nnumber of different values in a variable  \nAs we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values. We should handle these variables differently as they would result in many dummy variables.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.325918Z","iopub.execute_input":"2021-11-03T10:34:44.326280Z","iopub.status.idle":"2021-11-03T10:34:44.366345Z","shell.execute_reply.started":"2021-11-03T10:34:44.326226Z","shell.execute_reply":"2021-11-03T10:34:44.365731Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Only ps_car_11_cat has many distinct values, although it is still reasonable.","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series = None, tst_series = None, target = None,\n                 min_samples_leaf = 1, smoothing = 1, noise_level = 0):\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis = 1)\n    \n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\",\"count\"])\n    \n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    \n    # Apply average function to all target data\n    prior = target.mean()\n    \n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\",\"count\"], axis = 1, inplace = True)\n    \n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(trn_series.to_frame(trn_series.name), \n                             averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n                             on=trn_series.name,\n                             how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(tst_series.to_frame(tst_series.name),\n                             averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n                             on=tst_series.name,\n                             how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.367422Z","iopub.execute_input":"2021-11-03T10:34:44.367796Z","iopub.status.idle":"2021-11-03T10:34:44.387201Z","shell.execute_reply.started":"2021-11-03T10:34:44.367760Z","shell.execute_reply":"2021-11-03T10:34:44.386319Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Smoothing is computed like in the following paper by Daniele Micci-Barreca  \nhttps://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf  \n- trn_series : training categorical feature as a pd.Series\n- tst_series : test categorical feature as a pd.Series\n- target : target data as a pd.Series\n- min_samples_leaf (int) : minimum samples to take category average into account\n- smoothing (int) : smoothing effect to balance categorical average vs prior  ","metadata":{}},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], test[\"ps_car_11_cat\"], target = train.target,\n                                           min_samples_leaf = 100, smoothing = 10, noise_level = 0.01)\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis = 1, inplace = True)\nmeta.loc['ps_car_11_cat', 'keep'] = False    # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.388301Z","iopub.execute_input":"2021-11-03T10:34:44.389184Z","iopub.status.idle":"2021-11-03T10:34:44.747567Z","shell.execute_reply.started":"2021-11-03T10:34:44.389144Z","shell.execute_reply":"2021-11-03T10:34:44.746625Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# 6. Exploratory data visualization\n## Categorical variables\nLet's look into the categorical variables and the proportion of customers with target = 1","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:44.749051Z","iopub.execute_input":"2021-11-03T10:34:44.749865Z","iopub.status.idle":"2021-11-03T10:34:48.195345Z","shell.execute_reply.started":"2021-11-03T10:34:44.749798Z","shell.execute_reply":"2021-11-03T10:34:48.194235Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the variables with missing values, it is a good idea to __keep the missing values as a separate category value, instead of replacing them__ by the mode for instance.  \nThe customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim.\n\n## Interval variables\nChecking the correlations between interval variables. \n### Heatmap\na good way to visualize the correlation between variables. ","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    fig, ax = plt.subplots(figsize = (10, 10))\n    sns.heatmap(correlations, cmap = cmap, \n                vmax = 1.0, center = 0, fmt = '.2f', \n                square = True, linewidths = .5, annot = True, \n                cbar_kws = {\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:48.197189Z","iopub.execute_input":"2021-11-03T10:34:48.197424Z","iopub.status.idle":"2021-11-03T10:34:49.059245Z","shell.execute_reply.started":"2021-11-03T10:34:48.197395Z","shell.execute_reply":"2021-11-03T10:34:49.058062Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"There are a strong correlations between the variables:\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)\n\n### seaborn \nhas some handy plots to visualize the (linear) relationship between variables.  \n\n### pairplot\nWe could use a pairplot to visualize the relationship between the variables.  \nBut because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.  \nNOTE: The author took a sample of the train data to speed up the process.","metadata":{}},{"cell_type":"code","source":"s = train.sample(frac = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:49.060977Z","iopub.execute_input":"2021-11-03T10:34:49.061216Z","iopub.status.idle":"2021-11-03T10:34:49.095794Z","shell.execute_reply.started":"2021-11-03T10:34:49.061181Z","shell.execute_reply":"2021-11-03T10:34:49.094455Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### ps_reg_02 & ps_reg_03\nAs the regression line shows, there is a linear relationship between these variables.  \nThanks to the hue parameter we can see that the regression lines for target=0 and target=1 are the same.","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x = 'ps_reg_02', y = 'ps_reg_03', data = s, hue = 'target', palette = 'Set1', scatter_kws = {'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:49.097838Z","iopub.execute_input":"2021-11-03T10:34:49.098097Z","iopub.status.idle":"2021-11-03T10:34:50.874432Z","shell.execute_reply.started":"2021-11-03T10:34:49.098064Z","shell.execute_reply":"2021-11-03T10:34:50.873302Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### ps_reg_12 & ps_reg_13","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:50.876138Z","iopub.execute_input":"2021-11-03T10:34:50.876374Z","iopub.status.idle":"2021-11-03T10:34:52.628939Z","shell.execute_reply.started":"2021-11-03T10:34:50.876343Z","shell.execute_reply":"2021-11-03T10:34:52.627866Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### ps_car_12 & ps_car_14","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:52.631000Z","iopub.execute_input":"2021-11-03T10:34:52.631349Z","iopub.status.idle":"2021-11-03T10:34:54.434784Z","shell.execute_reply.started":"2021-11-03T10:34:52.631301Z","shell.execute_reply":"2021-11-03T10:34:54.433504Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### ps_car_13 & ps_car_15","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:54.436827Z","iopub.execute_input":"2021-11-03T10:34:54.437100Z","iopub.status.idle":"2021-11-03T10:34:56.190739Z","shell.execute_reply.started":"2021-11-03T10:34:54.437068Z","shell.execute_reply":"2021-11-03T10:34:56.189552Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Now, how can we decide which of the correlated variables to keep?  \nWe could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions.  \nBut as the number of correlated variables is rather low, we will let the model do the heavy-lifting.  \n\n## Checking the correlations between ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:56.192344Z","iopub.execute_input":"2021-11-03T10:34:56.192575Z","iopub.status.idle":"2021-11-03T10:34:57.807710Z","shell.execute_reply.started":"2021-11-03T10:34:56.192546Z","shell.execute_reply":"2021-11-03T10:34:57.806326Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"For the ordinal variables we do not see many correlations. We could, on the other hand, look at how the distributions are when grouping by the target value.\n\n\n# 7. Feature engineering\n## Creating dummy variables\nThe values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:57.809550Z","iopub.execute_input":"2021-11-03T10:34:57.810463Z","iopub.status.idle":"2021-11-03T10:34:57.959842Z","shell.execute_reply.started":"2021-11-03T10:34:57.810410Z","shell.execute_reply":"2021-11-03T10:34:57.958707Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"So, creating dummy variables adds 52 variables to the training set.\n\n## Creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:57.961334Z","iopub.execute_input":"2021-11-03T10:34:57.961594Z","iopub.status.idle":"2021-11-03T10:34:58.475743Z","shell.execute_reply.started":"2021-11-03T10:34:57.961561Z","shell.execute_reply":"2021-11-03T10:34:58.474482Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"This adds extra interaction variables to the train data. Thanks to the __get_feature_names__ method we can assign column names to these new variables.\n\n\n# 8. Feature selection\n## Removing features with low or zero variance\nPersonally, I prefer to let the classifier algorithm chose which features to keep. But there is one thing that we can do ourselves. That is removing features with no or a very low variance. Sklearn has a handy method to do that: __VarianceThreshold__. By default it removes features with zero variance. This will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps. But if we would remove features with less than 1% variance, we would remove 31 variables.","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:58.477396Z","iopub.execute_input":"2021-11-03T10:34:58.477654Z","iopub.status.idle":"2021-11-03T10:34:59.324560Z","shell.execute_reply.started":"2021-11-03T10:34:58.477608Z","shell.execute_reply":"2021-11-03T10:34:59.323336Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"We would lose rather many variables if we would select based on variance. But because we do not have so many variables, we'll let the classifier chose. For data sets with many more variables this could reduce the processing time.\n\nSklearn also comes with other feature selection methods. One of these methods is __SelectFromModel__ in which you let another classifier select the best features and continue with these\n\n## Selecting features with a Random Forest and SelectFromModel  \nHere we'll base feature selection on the feature importances of a random forest. With Sklearn's SelectFromModel you can then specify how many variables you want to keep. You can set a threshold on the level of feature importance manually. But we'll simply select the top 50% best variables.\n\nBelow code is borrowed from: https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch04/ch04.ipynb  \n(Must read!)","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:34:59.326567Z","iopub.execute_input":"2021-11-03T10:34:59.326923Z","iopub.status.idle":"2021-11-03T10:46:34.918065Z","shell.execute_reply.started":"2021-11-03T10:34:59.326888Z","shell.execute_reply":"2021-11-03T10:46:34.903128Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"With __SelectFromModel__, we can specify which prefit classifier to use and what the threshold is for the feature importances. With the get_support method we can then limit the number of variables in the train data.","metadata":{}},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:46:34.919784Z","iopub.execute_input":"2021-11-03T10:46:34.920048Z","iopub.status.idle":"2021-11-03T10:46:36.082734Z","shell.execute_reply.started":"2021-11-03T10:46:34.920017Z","shell.execute_reply":"2021-11-03T10:46:36.081706Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"train = train[selected_vars + ['target']]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:46:36.084226Z","iopub.execute_input":"2021-11-03T10:46:36.084477Z","iopub.status.idle":"2021-11-03T10:46:36.161097Z","shell.execute_reply.started":"2021-11-03T10:46:36.084445Z","shell.execute_reply":"2021-11-03T10:46:36.159847Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# 7. Feature scaling\nAs mentioned before, we can apply standard scaling to the training data. Some classifiers perform better when this is done.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:46:36.162803Z","iopub.execute_input":"2021-11-03T10:46:36.163044Z","iopub.status.idle":"2021-11-03T10:46:36.943535Z","shell.execute_reply.started":"2021-11-03T10:46:36.163014Z","shell.execute_reply":"2021-11-03T10:46:36.942650Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}