{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credit card fraud detection\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:05:58.497610Z","iopub.execute_input":"2022-01-19T11:05:58.498115Z","iopub.status.idle":"2022-01-19T11:05:58.526197Z","shell.execute_reply.started":"2022-01-19T11:05:58.498027Z","shell.execute_reply":"2022-01-19T11:05:58.525580Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:05:58.527407Z","iopub.execute_input":"2022-01-19T11:05:58.527843Z","iopub.status.idle":"2022-01-19T11:06:02.810046Z","shell.execute_reply.started":"2022-01-19T11:05:58.527804Z","shell.execute_reply":"2022-01-19T11:06:02.809178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Checking the target classes","metadata":{}},{"cell_type":"code","source":"count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:02.811954Z","iopub.execute_input":"2022-01-19T11:06:02.812416Z","iopub.status.idle":"2022-01-19T11:06:03.160732Z","shell.execute_reply.started":"2022-01-19T11:06:02.812373Z","shell.execute_reply":"2022-01-19T11:06:03.159897Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Setting our input and target variables + resampling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time','Amount'], axis = 1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:08:25.593163Z","iopub.execute_input":"2022-01-19T11:08:25.593693Z","iopub.status.idle":"2022-01-19T11:08:25.713422Z","shell.execute_reply.started":"2022-01-19T11:08:25.593631Z","shell.execute_reply":"2022-01-19T11:08:25.712573Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X = data.iloc[:, data.columns != 'Class']\ny = data.iloc[:, data.columns == 'Class']","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:09:10.142964Z","iopub.execute_input":"2022-01-19T11:09:10.143241Z","iopub.status.idle":"2022-01-19T11:09:10.174538Z","shell.execute_reply.started":"2022-01-19T11:09:10.143213Z","shell.execute_reply":"2022-01-19T11:09:10.173558Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Number of data points in the minority class\nnumber_records_fraud = len(data[data.Class == 1])\nfraud_indices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormal_indices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number ( number_records_fraud)\nrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices, :]\n\nX_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\ny_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1]) / len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:09:34.007074Z","iopub.execute_input":"2022-01-19T11:09:34.008026Z","iopub.status.idle":"2022-01-19T11:09:34.062116Z","shell.execute_reply.started":"2022-01-19T11:09:34.007984Z","shell.execute_reply":"2022-01-19T11:09:34.061504Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data into train and test set.  \n## Cross validation will be used when calculating accuracies","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Whole dataset\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n\nprint(\"Number transactions train dataset: \", len(X_train))\nprint(\"Number transactions test dataset: \", len(X_test))\nprint(\"Total number of transactions: \", len(X_train)+len(X_test))\n\n# Undersampled dataset\nX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, y_undersample,test_size = 0.3, random_state = 0)\n\nprint(\"\")\nprint(\"Number transactions train dataset: \", len(X_train_undersample))\nprint(\"Number transactions test dataset: \", len(X_test_undersample))\nprint(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:10:15.220082Z","iopub.execute_input":"2022-01-19T11:10:15.220350Z","iopub.status.idle":"2022-01-19T11:10:15.373040Z","shell.execute_reply.started":"2022-01-19T11:10:15.220322Z","shell.execute_reply":"2022-01-19T11:10:15.372021Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression classifier - Undersampled data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:17:04.601595Z","iopub.execute_input":"2022-01-19T11:17:04.602399Z","iopub.status.idle":"2022-01-19T11:17:04.607819Z","shell.execute_reply.started":"2022-01-19T11:17:04.602361Z","shell.execute_reply":"2022-01-19T11:17:04.606970Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def printing_Kfold_scores(x_train_data,y_train_data):\n    fold = KFold(len(y_train_data),n_splits=5,shuffle=False) \n    \n    # Different C parameters\n    c_param_range = [0.01, 0.1, 1, 10, 100]\n    \n    results_table = pd.DataFrame(index = range(len(c_param_range), 2), columns = ['C_parameter', 'Mean recall score'])\n    results_table['C_parameter'] = c_param_range\n    \n    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n    j = 0\n    for c_param in c_param_range:\n        print('---------------------------------------------------')\n        print('C parameter: ', c_param)\n        print('---------------------------------------------------')\n        print(' ')\n        \n        recal_accs = []\n        for iteration, indices in enumerate(fold, start = 1):\n            # Call the logistic regression model with a certain C parameter\n            lr = LogisticRegression(C = c_param, penalty = '11')\n            \n            # Use the training data to fit the model.\n            # In this case, we use the portion of the fold to train the model with indices[0].\n            # We then predict on the portion assigned as the 'test cross validation' with indices[1]\n            lr.fit(x_train_data.iloc[indices[0],:], y_train_data.iloc[indices[0], :].values.ravel())\n            \n            # Predict values using the test indices in the training data\n            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n            \n            # Calculate the recall score and append it to a list for a recall scores representing the current c_parameter\n            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values, y_pred_undersample)\n            recall_accs.append(recall_acc)\n            print('Iteration ', iteration, ': recall score = ', recall_acc)\n            \n        # The mean value of those recall scores is the metric we want to save and get hold of.\n        results_table.ix[j, 'Mean recall score'] = np.mean(recall_accs)\n        j += 1\n        print('')\n        print('Mean recall score ', np.mean(recall_accs))\n        print('')\n            \n    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n        \n    # Finally, we can check which C parameter is the best amongst the chosen.\n    print('*******************************************************************************')\n    print('Best model to choose from cross validation is with C parameter = ', best_c)\n    print('*******************************************************************************')\n        \n    return best_c","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:12:57.312382Z","iopub.execute_input":"2022-01-19T11:12:57.313254Z","iopub.status.idle":"2022-01-19T11:12:57.327967Z","shell.execute_reply.started":"2022-01-19T11:12:57.313209Z","shell.execute_reply":"2022-01-19T11:12:57.327220Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"best_c = printing_Kfold_scores(X_train_undersample, y_train_undersample)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:12:59.883244Z","iopub.execute_input":"2022-01-19T11:12:59.883524Z","iopub.status.idle":"2022-01-19T11:12:59.923939Z","shell.execute_reply.started":"2022-01-19T11:12:59.883496Z","shell.execute_reply":"2022-01-19T11:12:59.922835Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix',\n                          cmap = plt.cm.Blues):\n    \n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting 'normalization = True'.\n    \"\"\"\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    ticks_marks = np.arrange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis = 1) [:, np.newaxis]\n    else:\n        1\n    \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                horizontalalignment = \"center\",\n                color = \"white\" if cm[i, j] > thresh else \"black\")\n        \n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:04.201593Z","iopub.status.idle":"2022-01-19T11:06:04.202183Z","shell.execute_reply.started":"2022-01-19T11:06:04.202006Z","shell.execute_reply":"2022-01-19T11:06:04.202026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n# dataset\nlr = LogisticRegression(C = best_c, penalty = 'l1')\nlr.fit(X_train_undersample, y_train_undersample.values.ravel())\ny_pred_undersample = lr.predict(X_test_undersample.values)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test_undersample, y_pred_undersample)\nnp.set_printoptions(precision = 2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix,\n                      classes = class_names,\n                      title = 'Confusion matrix')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:06:04.203138Z","iopub.status.idle":"2022-01-19T11:06:04.203784Z","shell.execute_reply.started":"2022-01-19T11:06:04.203498Z","shell.execute_reply":"2022-01-19T11:06:04.203528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n# dataset\nlr = LogisticRegression(C = best_c, penalty = 'l1')\nlr.fit(X_train_undersample,y_train_undersample.values.ravel())\ny_pred = lr.predict(X_test.values)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test,y_pred)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC CURVE\nlr = LogisticRegression(C = best_c, penalty = 'l1')\ny_pred_undersample_score = lr.fit(X_train_undersample,y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)\n\nfpr, tpr, thresholds = roc_curve(y_test_undersample.values.ravel(),y_pred_undersample_score)\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression classifier - Skewed data","metadata":{}},{"cell_type":"code","source":"best_c = printing_Kfold_scores(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n# dataset\nlr = LogisticRegression(C = best_c, penalty = 'l1')\nlr.fit(X_train,y_train.values.ravel())\ny_pred_undersample = lr.predict(X_test.values)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test,y_pred_undersample)\nnp.set_printoptions(precision=2)\n\nprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Before continuing... changing classification threshold.","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(C = 0.01, penalty = 'l1')\nlr.fit(X_train_undersample,y_train_undersample.values.ravel())\ny_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize=(10,10))\n\nj = 1\nfor i in thresholds:\n    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall)\n    np.set_printoptions(precision=2)\n\n    print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n\n    # Plot non-normalized confusion matrix\n    class_names = [0,1]\n    plot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import cycle\n\nlr = LogisticRegression(C = 0.01, penalty = 'l1')\nlr.fit(X_train_undersample,y_train_undersample.values.ravel())\ny_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black'])\n\nplt.figure(figsize=(5,5))\n\nj = 1\nfor i,color in zip(thresholds,colors):\n    y_test_predictions_prob = y_pred_undersample_proba[:,1] > i\n    \n    precision, recall, thresholds = precision_recall_curve(y_test_undersample,y_test_predictions_prob)\n    \n    # Plot Precision-Recall curve\n    plt.plot(recall, precision, color=color,\n                 label='Threshold: %s'%i)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('Precision-Recall example')\n    plt.legend(loc=\"lower left\")","metadata":{},"execution_count":null,"outputs":[]}]}